{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: IMDb Sentiment Analysis with ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/jupyter-1.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: clearml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.16.5)\n",
      "Requirement already satisfied: attrs>=18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (24.2.0)\n",
      "Requirement already satisfied: furl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.1.3)\n",
      "Requirement already satisfied: jsonschema>=2.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.0.2)\n",
      "Requirement already satisfied: pathlib2>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.3.7.post1)\n",
      "Requirement already satisfied: Pillow>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (10.4.0)\n",
      "Requirement already satisfied: psutil>=3.4.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (6.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.9.0.post0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.32.3)\n",
      "Requirement already satisfied: six>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.2.3)\n",
      "Requirement already satisfied: pyjwt<2.9.0,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (2.8.0)\n",
      "Requirement already satisfied: referencing<0.40 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from clearml) (0.35.1)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from furl>=2.0.0->clearml) (1.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonschema>=2.6.0->clearml) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonschema>=2.6.0->clearml) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20.0->clearml) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20.0->clearml) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.20.0->clearml) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install clearml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=81e355685bf64d779023f5888195dd96\n",
      "2024-12-19 14:51:31,234 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/bb33ce1736ad44cbbbb6238137b2528a/experiments/81e355685bf64d779023f5888195dd96/output/log\n"
     ]
    }
   ],
   "source": [
    "from clearml import Task\n",
    "# Then initialize your new task\n",
    "task = Task.init(\n",
    "    project_name=\"IMDb Sentiment Analysis\", \n",
    "    task_name=\"Multi-Model Comparison\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "Total samples: 50000\n",
      "Null values:\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.8894\n",
      "Precision: 0.8831\n",
      "Recall: 0.8976\n",
      "F1 Score: 0.8903\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.8518\n",
      "Precision: 0.8609\n",
      "Recall: 0.8392\n",
      "F1 Score: 0.8499\n",
      "\n",
      "Comparative Model Performance:\n",
      "\n",
      "Logistic Regression:\n",
      "Accuracy: 0.8894\n",
      "Precision: 0.8831\n",
      "Recall: 0.8976\n",
      "F1_score: 0.8903\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.8518\n",
      "Precision: 0.8609\n",
      "Recall: 0.8392\n",
      "F1_score: 0.8499\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from clearml import Task\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load IMDb dataset and perform initial checks\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"../data/IMDB-Dataset.csv\")\n",
    "    \n",
    "    # Data validation\n",
    "    print(\"Dataset Information:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Null values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, max_features=5000, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess text data using TF-IDF vectorization\n",
    "    \"\"\"\n",
    "    X = df['review']\n",
    "    y = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Vectorization\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, \n",
    "        stop_words='english'\n",
    "    )\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_vec, X_test_vec, y_train, y_test\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='binary'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, task_logger):\n",
    "    \"\"\"\n",
    "    Plot and log confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    # Save plot\n",
    "    plt.tight_layout()\n",
    "    confusion_matrix_path = f'{model_name}_confusion_matrix.png'\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    \n",
    "    # Log to ClearML\n",
    "    task_logger.report_image(\n",
    "        title=f'{model_name} Confusion Matrix', \n",
    "        series='Confusion Matrix', \n",
    "        local_path=confusion_matrix_path\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Initialize ClearML Task\n",
    "    task = Task.init(\n",
    "        project_name=\"IMDb Sentiment Analysis\", \n",
    "        task_name=\"Multi-Model Comparison\"\n",
    "    )\n",
    "    \n",
    "    # Log configuration\n",
    "    task.connect_configuration({\n",
    "        'vectorizer_max_features': 5000,\n",
    "        'test_size': 0.2,\n",
    "        'random_state': 42\n",
    "    })\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_data()\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Model training and evaluation\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        model_results = evaluate_model(y_test, y_pred, name)\n",
    "        results[name] = model_results\n",
    "        \n",
    "        # Plot and log confusion matrix\n",
    "        plot_confusion_matrix(y_test, y_pred, name, task.get_logger())\n",
    "        \n",
    "        # Log metrics to ClearML\n",
    "        for metric, value in model_results.items():\n",
    "            task.get_logger().report_scalar(\n",
    "                title=name, \n",
    "                series=metric, \n",
    "                iteration=1, \n",
    "                value=value\n",
    "            )\n",
    "    \n",
    "    # Comparative analysis\n",
    "    print(\"\\nComparative Model Performance:\")\n",
    "    for model, metrics in results.items():\n",
    "        print(f\"\\n{model}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clearml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
